{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab04008c",
   "metadata": {},
   "source": [
    "importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470463c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf27ff",
   "metadata": {},
   "source": [
    "use requests to fetch the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3f6157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = requests.get(\"https://wuzzuf.net/search/jobs/?q=python&a=hpb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95bb3a6",
   "metadata": {},
   "source": [
    "save page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6705ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f12ac",
   "metadata": {},
   "source": [
    "create soup to parce content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67bbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(src , \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419f773",
   "metadata": {},
   "source": [
    "find the elements containing info we need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83f9dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titel = []\n",
    "jobs_titels = soup.find_all(\"h2\" , {\"class\" : \"css-m604qf\"}) \n",
    "\n",
    "job_location = []\n",
    "jobs_location = soup.find_all(\"span\" , {\"class\":\"css-5wys0k\"})\n",
    "\n",
    "company_name = []\n",
    "company_names = soup.find_all(\"a\" , {\"class\" : \"css-17s97q8\"})\n",
    "\n",
    "job_skill = []\n",
    "jobs_skills = soup.find_all(\"div\" , {\"class\" : \"css-y4udm8\"})\n",
    "\n",
    "links = []\n",
    "\n",
    "Date = []\n",
    "Posted_New = soup.find_all(\"div\" , {\"class\" , \"css-4c4ojb\"})\n",
    "Posted_Old = soup.find_all(\"div\" , {\"class\" , \"css-do6t5g\"})\n",
    "Posted = [*Posted_New , *Posted_Old]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd59697",
   "metadata": {},
   "source": [
    "loop over returned lests to extract needed info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f1fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(jobs_titels)):\n",
    "    job_titel.append(jobs_titels[i].text)\n",
    "    links.append(jobs_titels[i].find(\"a\").attrs['href'])\n",
    "    job_location.append(jobs_location[i].text)\n",
    "    company_name.append(company_names[i].text)\n",
    "    job_skill.append(jobs_skills[i].text)\n",
    "    Date.append(Posted[i].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e41b15",
   "metadata": {},
   "source": [
    "create csv file and fill it with values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d978eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [job_titel , job_location, Date ,company_name ,job_skill , links ]\n",
    "exported = zip_longest(*file_list)\n",
    "\n",
    "with open(\"jobs_info.csv\" , \"w\") as jobs:\n",
    "    wr=csv.writer(jobs)\n",
    "    wr.writerow([\"job titel\" , \"job location\" , \"Date\" , \"company name\" , \"job skills\" , \"links\"])\n",
    "    wr.writerows(exported)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
